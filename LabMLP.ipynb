{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "These two code sections demonstrate the implementation of multilayer perceptrons (MLPs) for different classification tasks.\n",
        "\n",
        "The first part is an MLP model applied to the XOR function, a classic example to showcase neural networks' ability to learn non-linear relationships. The model has one hidden layer with 2 neurons, using the ReLU activation function and the Adam optimizer. After training on the XOR inputs, the model outputs predictions, calculates accuracy, and displays weights and biases of the network layers.\n",
        "\n",
        "The second part involves an MLP for the MNIST digit classification task. This model includes two hidden layers with 128 and 64 neurons, both using ReLU, and a 10-neuron output layer with a softmax activation for multi-class classification. The model is compiled with `categorical_crossentropy` loss and the Adam optimizer, trained over 5 epochs, and evaluated for accuracy on the test set, demonstrating its ability to handle complex, large-scale data."
      ],
      "metadata": {
        "id": "zgIgxy8tmnHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP pour XOR**"
      ],
      "metadata": {
        "id": "zy_RCwxeO5Dc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ohZPIDOOyCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee891313-7eb6-4e45-c4f7-e9a4ac2ef5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prédictions du MLP sur les données d'entraînement : [0 1 1 0]\n",
            "Exactitude du modèle : 1.0\n",
            "Poids entre l'entrée et la couche cachée : [[ 2.18846795 -1.86266266]\n",
            " [ 2.18828852 -1.86278539]]\n",
            "Poids entre la couche cachée et la sortie : [[-1.99501509]\n",
            " [-2.32648246]]\n",
            "Biais de la couche cachée : [-2.18824793  1.86263993]\n",
            "Biais de la couche de sortie : [1.86035519]\n"
          ]
        }
      ],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Définition des données d'entraînement\n",
        "# On définit les entrées et les sorties pour la fonction XOR\n",
        "# XOR(0,0) = 0, XOR(0,1) = 1, XOR(1,0) = 1, XOR(1,1) = 0\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y = [0, 1, 1, 0]\n",
        "\n",
        "# Création du modèle MLP (Perceptron Multicouche)\n",
        "# Le modèle a 2 neurones dans la couche cachée, avec la fonction d'activation 'relu'\n",
        "# et l'algorithme de rétropropagation 'adam'\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='relu', solver='adam', max_iter=10000)\n",
        "\n",
        "# Entraînement du modèle sur les données d'entraînement\n",
        "mlp.fit(X, y)\n",
        "\n",
        "# Prédiction sur les données d'entraînement pour évaluer les performances\n",
        "y_pred = mlp.predict(X)\n",
        "\n",
        "# Calcul de l'exactitude du modèle\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"Prédictions du MLP sur les données d'entraînement :\", y_pred)\n",
        "print(\"Exactitude du modèle :\", accuracy)\n",
        "\n",
        "# Affichage des poids des couches du réseau de neurones\n",
        "print(\"Poids entre l'entrée et la couche cachée :\", mlp.coefs_[0])\n",
        "print(\"Poids entre la couche cachée et la sortie :\", mlp.coefs_[1])\n",
        "\n",
        "# Affichage des biais des neurones\n",
        "print(\"Biais de la couche cachée :\", mlp.intercepts_[0])\n",
        "print(\"Biais de la couche de sortie :\", mlp.intercepts_[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP MNIST**"
      ],
      "metadata": {
        "id": "TfWP669UvenS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### ESSAI 1\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normaliser les données (0-255 -> 0-1)\n",
        "x_train = x_train.reshape(60000, 28 * 28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(10000, 28 * 28).astype('float32') / 255.0\n",
        "\n",
        "# Encoder les labels en one-hot vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Construction du modèle MLP\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle avec 5 itérations et par lot de 64 exemples\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "# Évaluation du modèle\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Précision sur l\\'ensemble de test:', test_acc)\n"
      ],
      "metadata": {
        "id": "-fsRwQjHo31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### ESSAI 2  : EARLY STOPPING + CHANGE;ENT DE L'OPTIMIZER ET LEARNING RATE\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "#### ajout de early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normaliser les données (0-255 -> 0-1)\n",
        "x_train = x_train.reshape(60000, 28 * 28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(10000, 28 * 28).astype('float32') / 255.0\n",
        "\n",
        "# Encoder les labels en one-hot vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Construction du modèle MLP\n",
        "#Couche d'entrée : 784 neurones (correspondant à chaque pixel de l'image).\n",
        "#Première couche cachée :  128 neurones avec la fonction d'activation ReLU.\n",
        "#Deuxième couche cachée :  64 neurones avec la fonction d'activation ReLU.\n",
        "#Couche de sortie : 10 neurones (correspondant aux 10 classes), avec une activation softmax.\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compilation du modèle\n",
        "#Utiliser la fonction de coût categorical crossentropy pour la classification multi-classes.\n",
        "#Optimiser le modèle avec l'algorithme Adam.\n",
        "#Entraîner le modèle sur l'ensemble d'entraînement\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # pERMET\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle qvec 5 éteration et par lot de 64 exemples\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Évaluation du modèle\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Précision sur l\\'ensemble de test:', test_acc)\n"
      ],
      "metadata": {
        "id": "V-KcebGPvkWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ESSAI 3 : AJOUT D'UN DROPOUT LAYER + 10 EPOCH\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normaliser les données (0-255 -> 0-1)\n",
        "x_train = x_train.reshape(60000, 28 * 28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(10000, 28 * 28).astype('float32') / 255.0\n",
        "\n",
        "# Encoder les labels en one-hot vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Construction du modèle MLP\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.add(layers.Dropout(0.3))  # Drop 30% of the neurons at random\n",
        "\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle avec 5 itérations et par lot de 64 exemples\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64)\n",
        "\n",
        "# Évaluation du modèle\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Précision sur l\\'ensemble de test:', test_acc)\n"
      ],
      "metadata": {
        "id": "FEvr7dUvpKnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage du résumé du modèle pour voir l'architecture et les paramètres\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Nsf5eTK2Ay2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}