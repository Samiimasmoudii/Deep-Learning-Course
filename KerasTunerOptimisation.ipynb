{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samiimasmoudii/Deep-Learning-Course/blob/main/KerasTunerOptimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   #RandomSearch with Keras Tuner#"
      ],
      "metadata": {
        "id": "MphESgi3M7_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEoAgkn0-EVv",
        "outputId": "1f984a95-797e-4bcb-a2a3-0942f30531ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.8.30)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w6jDKmrMssu",
        "outputId": "8a69da3b-b102-485d-d9a2-ee753ac39d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 05m 53s]\n",
            "val_accuracy: 0.7161500155925751\n",
            "\n",
            "Best val_accuracy So Far: 0.7161500155925751\n",
            "Total elapsed time: 00h 22m 07s\n",
            "\n",
            "Le meilleur nombre de filtres pour la première couche : 64\n",
            "Le meilleur nombre de filtres pour la deuxième couche : 64\n",
            "La meilleure taille de kernel pour la première couche : 3\n",
            "Taux d'apprentissage optimal : 0.001\n",
            "\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 19s 14ms/step - loss: 1.5835 - accuracy: 0.4209 - val_loss: 1.3057 - val_accuracy: 0.5348\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2843 - accuracy: 0.5404 - val_loss: 1.1198 - val_accuracy: 0.6076\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1721 - accuracy: 0.5856 - val_loss: 1.0342 - val_accuracy: 0.6368\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1000 - accuracy: 0.6125 - val_loss: 0.9802 - val_accuracy: 0.6591\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 18s 14ms/step - loss: 1.0331 - accuracy: 0.6363 - val_loss: 0.9309 - val_accuracy: 0.6754\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9846 - accuracy: 0.6519 - val_loss: 0.9032 - val_accuracy: 0.6870\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9406 - accuracy: 0.6703 - val_loss: 0.9398 - val_accuracy: 0.6776\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.9097 - accuracy: 0.6816 - val_loss: 0.8496 - val_accuracy: 0.7088\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8809 - accuracy: 0.6906 - val_loss: 0.8880 - val_accuracy: 0.6925\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 17s 14ms/step - loss: 0.8503 - accuracy: 0.7015 - val_loss: 0.8576 - val_accuracy: 0.7067\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8619 - accuracy: 0.7064\n",
            "Test Accuracy: 0.71\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Charger le dataset CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normaliser les données (échelle entre 0 et 1)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Encodage one-hot des labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Définition de la fonction pour construire le modèle\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Convolutional Layer 1\n",
        "    model.add(tf.keras.layers.Conv2D(\n",
        "        filters=hp.Choice('filters_1', values=[32, 64, 96]),\n",
        "        kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
        "        activation='relu',\n",
        "        input_shape=(32, 32, 3)\n",
        "    ))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(hp.Choice('dropout_1', values=[0.2, 0.3, 0.4])))\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    model.add(tf.keras.layers.Conv2D(\n",
        "        filters=hp.Choice('filters_2', values=[64, 128]),\n",
        "        kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(hp.Choice('dropout_2', values=[0.2, 0.3, 0.4])))\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=hp.Choice('dense_units', values=[128, 256, 512]),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dropout(hp.Choice('dropout_3', values=[0.3, 0.4, 0.5])))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))  # 10 classes\n",
        "\n",
        "    # Compiler le modèle\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Définir le tuner Keras\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,  # Nombre d'essais de combinaisons d'hyperparamètres\n",
        "    executions_per_trial=2,  # Nombre d'exécutions par essai pour la robustesse\n",
        "    directory='cifar10_tuning',\n",
        "    project_name='cnn_tuner'\n",
        ")\n",
        "\n",
        "# Recherche des hyperparamètres\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
        ")\n",
        "\n",
        "# Afficher le meilleur hyperparamètre trouvé\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"\"\"\n",
        "Le meilleur nombre de filtres pour la première couche : {best_hps.get('filters_1')}\n",
        "Le meilleur nombre de filtres pour la deuxième couche : {best_hps.get('filters_2')}\n",
        "La meilleure taille de kernel pour la première couche : {best_hps.get('kernel_size_1')}\n",
        "Taux d'apprentissage optimal : {best_hps.get('learning_rate')}\n",
        "\"\"\")\n",
        "\n",
        "# Entraîner le meilleur modèle\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Évaluer le modèle sur les données de test\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simulation de Grid Search with Tuner**"
      ],
      "metadata": {
        "id": "XtOJ4TuwzoF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Charger et préparer les données CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalisation des données\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Encodage one-hot des labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Fonction pour construire le modèle\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    # Couche convolutionnelle avec recherche d'hyperparamètres\n",
        "    model.add(Conv2D(\n",
        "        filters=hp.Choice('num_filters', [32, 64]),   # Choix des nombres de filtres\n",
        "        kernel_size=hp.Choice('kernel_size', [3, 5]), # Taille du noyau\n",
        "        activation='relu',\n",
        "        input_shape=(32, 32, 3)\n",
        "    ))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.4]))) # Taux de dropout\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.4])))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compilation avec un taux d'apprentissage variable\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp.Choice('learning_rate', [0.001, 0.0001])\n",
        "        ),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Initialisation du tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy', # Objectif à maximiser\n",
        "    max_trials=16,            # Nombre total de combinaisons (GridSearch simulé)\n",
        "    executions_per_trial=1,   # Répétition des essais pour plus de stabilité\n",
        "    directory='grid_search_tuner',\n",
        "    project_name='cifar10_tuning'\n",
        ")\n",
        "\n",
        "# Définir les données pour la recherche\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    validation_split=0.2,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Afficher les meilleurs hyperparamètres\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"\\nMeilleurs hyperparamètres :\")\n",
        "print(f\"Nombre de filtres : {best_hps.get('num_filters')}\")\n",
        "print(f\"Taille du noyau : {best_hps.get('kernel_size')}\")\n",
        "print(f\"Taux de dropout : {best_hps.get('dropout_rate')}\")\n",
        "print(f\"Taux d'apprentissage : {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# Entraîner le meilleur modèle\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Évaluation finale sur le jeu de test\n",
        "test_loss, test_acc = best_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"\\nPrécision finale sur les données de test : {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "qYdLSWEFzzO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00de3347-38df-4603-c6ca-db6b919f2164"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 16 Complete [00h 00m 29s]\n",
            "val_accuracy: 0.6025000214576721\n",
            "\n",
            "Best val_accuracy So Far: 0.6360999941825867\n",
            "Total elapsed time: 00h 09m 11s\n",
            "\n",
            "Meilleurs hyperparamètres :\n",
            "Nombre de filtres : 64\n",
            "Taille du noyau : 5\n",
            "Taux de dropout : 0.2\n",
            "Taux d'apprentissage : 0.001\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 1.6389 - accuracy: 0.4083 - val_loss: 1.3972 - val_accuracy: 0.5118\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 1.3679 - accuracy: 0.5095 - val_loss: 1.2855 - val_accuracy: 0.5443\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 1.2692 - accuracy: 0.5479 - val_loss: 1.1903 - val_accuracy: 0.5872\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 1.1735 - accuracy: 0.5814 - val_loss: 1.1678 - val_accuracy: 0.5847\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 1.0976 - accuracy: 0.6101 - val_loss: 1.1030 - val_accuracy: 0.6155\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 1.0405 - accuracy: 0.6280 - val_loss: 1.0757 - val_accuracy: 0.6257\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.9897 - accuracy: 0.6457 - val_loss: 1.0712 - val_accuracy: 0.6300\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.9372 - accuracy: 0.6634 - val_loss: 1.0789 - val_accuracy: 0.6339\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.8965 - accuracy: 0.6792 - val_loss: 1.0993 - val_accuracy: 0.6281\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.8585 - accuracy: 0.6938 - val_loss: 1.0457 - val_accuracy: 0.6447\n",
            "\n",
            "Précision finale sur les données de test : 0.6424\n"
          ]
        }
      ]
    }
  ]
}