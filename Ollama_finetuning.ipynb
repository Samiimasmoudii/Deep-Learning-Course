{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URvB-671Uyxy"
      },
      "source": [
        "# **Step 1.** Install Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzDyBvmVnYZX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -upgrade\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install --no-deps packaging ninja einops flash-attn trl peft accelerate bitsandbytes\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qSIJyeAslCEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qnUA8cVU73A"
      },
      "source": [
        "# **Step 2.** Import Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sl1jjxFMeVx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BvjU9GZVBsl"
      },
      "source": [
        "# **Step 3.** Login to Your Hugging Face with hf_token. (write access token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "26c9a8375c7f471b8950907a9ff4c8c3",
            "cc8e1adaa36645eab4cf21f4de1389b5"
          ]
        },
        "id": "jMu8Tb8cVKpo",
        "outputId": "ea5b41a9-31b2-41cf-952f-20f84ecaa2b0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26c9a8375c7f471b8950907a9ff4c8c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz-0HCyjYoZ-"
      },
      "source": [
        "# **Step 4.** Convert JSON dataset to Llama3 finetuning format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6ZzbDEmYna3"
      },
      "outputs": [],
      "source": [
        "huggingface_user = \"samiimasmoudii\"\n",
        "dataset_name = \"finetuning_CV\"\n",
        "\n",
        "class Llama3InstructDataset:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.prompts = []\n",
        "        self.create_prompts()\n",
        "\n",
        "    def create_prompt(self, row):\n",
        "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{row['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>{row['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{row['output']}<|eot_id|>\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def create_prompts(self):\n",
        "        for row in self.data:\n",
        "            prompt = self.create_prompt(row)\n",
        "            self.prompts.append(prompt)\n",
        "\n",
        "    def get_dataset(self):\n",
        "        df = pd.DataFrame({'prompt': self.prompts})\n",
        "        return df\n",
        "\n",
        "def create_dataset_hf(dataset):\n",
        "    dataset.reset_index(drop=True, inplace=True)\n",
        "    return DatasetDict({\"train\": Dataset.from_pandas(dataset)})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('/content/dataset.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset = Llama3InstructDataset(data)\n",
        "    df = dataset.get_dataset()\n",
        "\n",
        "    processed_data_path = 'processed_data'\n",
        "    os.makedirs(processed_data_path, exist_ok=True)\n",
        "\n",
        "    llama3_dataset = create_dataset_hf(df)\n",
        "    llama3_dataset.save_to_disk(os.path.join(processed_data_path, \"llama3_dataset\"))\n",
        "    llama3_dataset.push_to_hub(f\"{huggingface_user}/{dataset_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_WiE8XYVXqQ"
      },
      "source": [
        "# **Step 5.** LoRa Finetuning Configurations\n",
        "- \"finetuned_model\" sets your models name on HF\n",
        "- \"num_train_epochs\" sets the number of epochs for training\n",
        "\n",
        "    (epoch = 1 pass through your entire dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQQN-uSUzB8h"
      },
      "outputs": [],
      "source": [
        "# Defining the configuration for the base model, LoRA and training\n",
        "config = {\n",
        "    \"hugging_face_username\":huggingface_user,\n",
        "    \"model_config\": {\n",
        "        \"base_model\":\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # The base model\n",
        "        \"finetuned_model\":\"llama-3-8b-Instruct-bnb-4bit-SamiCv\", # The finetuned model\n",
        "        \"max_seq_length\": 2048, # The maximum sequence length\n",
        "        \"dtype\":torch.float16, # The data type\n",
        "        \"load_in_4bit\": True, # Load the model in 4-bit\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "      \"r\": 16, # The number of LoRA layers 8, 16, 32, 64\n",
        "      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n",
        "      \"lora_alpha\":16, # The alpha value for LoRA\n",
        "      \"lora_dropout\":0, # The dropout value for LoRA\n",
        "      \"bias\":\"none\", # The bias for LoRA\n",
        "      \"use_gradient_checkpointing\":True, # Use gradient checkpointing\n",
        "      \"use_rslora\":False, # Use RSLora\n",
        "      \"use_dora\":False, # Use DoRa\n",
        "      \"loftq_config\":None # The LoFTQ configuration\n",
        "    },\n",
        "    \"training_dataset\":{\n",
        "        \"name\":f\"{huggingface_user}/{dataset_name}\", # The dataset name(huggingface/datasets)\n",
        "        \"split\":\"train\", # The dataset split\n",
        "        \"input_field\":\"prompt\", # The input field\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"per_device_train_batch_size\": 2, # The batch size\n",
        "        \"gradient_accumulation_steps\": 4, # The gradient accumulation steps\n",
        "        \"warmup_steps\": 5, # The warmup steps\n",
        "        \"max_steps\":0, # The maximum steps (0 if the epochs are defined)\n",
        "        \"num_train_epochs\": 10, # The number of training epochs(0 if the maximum steps are defined)\n",
        "        \"learning_rate\": 2e-4, # The learning rate\n",
        "        \"fp16\": not torch.cuda.is_bf16_supported(),  # The fp16\n",
        "        \"bf16\": torch.cuda.is_bf16_supported() ,# The bf16\n",
        "        \"logging_steps\": 1, # The logging steps\n",
        "        \"optim\" :\"adamw_8bit\", # The optimizer\n",
        "        \"weight_decay\" : 0.01,  # The weight decay\n",
        "        \"lr_scheduler_type\": \"linear\", # The learning rate scheduler\n",
        "        \"seed\" : 42, # The seed\n",
        "        \"output_dir\" : \"outputs\", # The output directory\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcpVNM_7ZGbi"
      },
      "source": [
        "# **Step 6.** Load Llama3-8B, QLoRA & Trainer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4wxJAgnM2W0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Loading the model and the tokinizer for the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = config.get(\"model_config\").get(\"base_model\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        ")\n",
        "\n",
        "# Setup for QLoRA/LoRA peft of the base model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = config.get(\"lora_config\").get(\"r\"),\n",
        "    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n",
        "    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n",
        "    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n",
        "    bias = config.get(\"lora_config\").get(\"bias\"),\n",
        "    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n",
        "    random_state = 42,\n",
        "    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n",
        "    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n",
        "    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n",
        ")\n",
        "\n",
        "# Loading the training dataset\n",
        "dataset_train = load_dataset(config.get(\"training_dataset\").get(\"name\"), split = config.get(\"training_dataset\").get(\"split\"))\n",
        "\n",
        "# Setting up the trainer for the model\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset_train,\n",
        "    dataset_text_field = config.get(\"training_dataset\").get(\"input_field\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
        "        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
        "        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n",
        "        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n",
        "        num_train_epochs= config.get(\"training_config\").get(\"num_train_epochs\"),\n",
        "        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n",
        "        fp16 = config.get(\"training_config\").get(\"fp16\"),\n",
        "        bf16 = config.get(\"training_config\").get(\"bf16\"),\n",
        "        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n",
        "        optim = config.get(\"training_config\").get(\"optim\"),\n",
        "        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n",
        "        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n",
        "        seed = 42,\n",
        "        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u-v9ArEnYZb"
      },
      "source": [
        "# **Step 7.** Train Your Finetuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI9mEQ7ZOUx2"
      },
      "outputs": [],
      "source": [
        "strainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSeaIIdVnYZb"
      },
      "source": [
        "# **Step 8.** Save Trainer Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWc-3iYuVKpq"
      },
      "outputs": [],
      "source": [
        "with open(\"trainer_stats.json\", \"w\") as f:\n",
        "    json.dump(trainer_stats, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTKdAmRnYZc"
      },
      "source": [
        "# **Step 9.** Save Finetuned Model & Push to HF Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzDLfiN-VKpq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgyNn-lLnYZc"
      },
      "source": [
        "# **Step 10.** Test your pretrained model in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "58fd3e614d8b4494b3fad58fe7c6cf12",
            "2c77cea8ae304c7ab1ac390673cb58b4",
            "fa789442f7204b8ca0849df2ae3af9b3",
            "facd90d32889498c8bba26fc44c993b1",
            "1478e86158d14986806d8ce6a4e79714",
            "a42f73d515a04cc4a4b09f50443022eb",
            "fd7348cbbe674ae396f2ee2f310fa666",
            "a74fd7a33f064728aba724fd6934b37d",
            "7c05429611a84587b74e8c27d3f8d3b8",
            "50bd6ec99f3847c1b4f1dd48abc6206c",
            "3a07bafc31b04802b6151869a7f6b94b"
          ]
        },
        "id": "ozVcalyP_JLs",
        "outputId": "52694b5f-cf86-459b-fabc-ec24fe5319c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58fd3e614d8b4494b3fad58fe7c6cf12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[\"systemYou are an AI task automator. You will take a users prompt and use first principle reasoning to break the prompt into tasks that you must accomplish within another chat. RESPOND TO THIS MESSAGE ONLY WITH A PYTHON FORMATTED LIST OF TASKS THAT YOU MUST COMPLETE TO TRUTHFULLY AND INTELLIGENTLY ACCOMPLISH THE USERS REQUEST. ASSUME YOU CAN SEARCH THE WEB, WRITE CODE, RUN CODE, DEBUG CODE, AND AUTOMATE ANYTHING ON THE USERS COMPUTER TO ACCOMPLISH THE PROMPT. CORRECT RESPONSE FORMAT: ['task 1', 'task 2', 'task 3']userGIve ME user data assistant\\n\\n['Obtain user input for prompt', 'Analyze user prompt for specific requirements', 'Determine intent behind user prompt', 'Generate task list based on user prompt']\"]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        "    )\n",
        "\n",
        "# Using FastLanguageModel for fast inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "system_prompt = f\"You are an AI task automator. You will take a users prompt and use first principle reasoning to break the prompt into tasks that you must accomplish within another chat. RESPOND TO THIS MESSAGE ONLY WITH A PYTHON FORMATTED LIST OF TASKS THAT YOU MUST COMPLETE TO TRUTHFULLY AND INTELLIGENTLY ACCOMPLISH THE USERS REQUEST. ASSUME YOU CAN SEARCH THE WEB, WRITE CODE, RUN CODE, DEBUG CODE, AND AUTOMATE ANYTHING ON THE USERS COMPUTER TO ACCOMPLISH THE PROMPT. CORRECT RESPONSE FORMAT: ['task 1', 'task 2', 'task 3']\"\n",
        "\n",
        "# Tokenizing the input and generating the output\n",
        "prompt = input('TYPE PROMPT TO LLAMA3: ')\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    f\"<|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}<|end_header_id|>\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXYSbe-wZXLK"
      },
      "source": [
        "Done by @samiimasmoudii"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1478e86158d14986806d8ce6a4e79714": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c9a8375c7f471b8950907a9ff4c8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_cc8e1adaa36645eab4cf21f4de1389b5"
          }
        },
        "2c77cea8ae304c7ab1ac390673cb58b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42f73d515a04cc4a4b09f50443022eb",
            "placeholder": "​",
            "style": "IPY_MODEL_fd7348cbbe674ae396f2ee2f310fa666",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "3a07bafc31b04802b6151869a7f6b94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50bd6ec99f3847c1b4f1dd48abc6206c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58fd3e614d8b4494b3fad58fe7c6cf12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c77cea8ae304c7ab1ac390673cb58b4",
              "IPY_MODEL_fa789442f7204b8ca0849df2ae3af9b3",
              "IPY_MODEL_facd90d32889498c8bba26fc44c993b1"
            ],
            "layout": "IPY_MODEL_1478e86158d14986806d8ce6a4e79714"
          }
        },
        "7c05429611a84587b74e8c27d3f8d3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a42f73d515a04cc4a4b09f50443022eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74fd7a33f064728aba724fd6934b37d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8e1adaa36645eab4cf21f4de1389b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fa789442f7204b8ca0849df2ae3af9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a74fd7a33f064728aba724fd6934b37d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c05429611a84587b74e8c27d3f8d3b8",
            "value": 0
          }
        },
        "facd90d32889498c8bba26fc44c993b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50bd6ec99f3847c1b4f1dd48abc6206c",
            "placeholder": "​",
            "style": "IPY_MODEL_3a07bafc31b04802b6151869a7f6b94b",
            "value": " 0/4 [00:00&lt;?, ?it/s]"
          }
        },
        "fd7348cbbe674ae396f2ee2f310fa666": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}