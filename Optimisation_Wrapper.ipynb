{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samiimasmoudii/Deep-Learning-Course/blob/main/Optimisation_Wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eKxnhrmO9x5"
      },
      "source": [
        "**Optimisation des architectures et des hyperparamètres**\n",
        "Optimisation d'un CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3zEajgGh3PP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b8ecbd-fee5-4afc-9580-244541d7670c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation terminée.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Liste des packages à installer\n",
        "packages = [\"tensorflow\", \"scikit-learn\"]\n",
        "\n",
        "for package in packages:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"Installation terminée.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIfwBfsjio3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372017fe-f00f-4589-f1af-a5b20aa00a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.1\n",
            "scikit-learn version: 1.5.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import sklearn\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOWBHnVSUsyR"
      },
      "source": [
        "Optimisation par **GridSearch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6zO-UWhfdP"
      },
      "source": [
        "Importer les bibliothèques nécessaires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MN-ZWTFU9i6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laUSk3ZFvNpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93315ebf-a731-4cba-dac6-1bf53af02819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P0rcstLfQsu"
      },
      "source": [
        "**Étape 1** : Préparer les données Utilisons le dataset CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-eOOwOJf48V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20305e2-15c1-4df4-fb90-231916a04ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Charger les données\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normaliser les données\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Convertir les labels en encodage one-hot\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdBjH8nagBwA"
      },
      "source": [
        "**Étape 2** : Construire le modèle de base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-E9xm0NgUsQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def create_model(learning_rate=0.01, dropout_rate=0.5, num_filters=32, kernel_size=3):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(32, 32, 3))) # IMage size is 32*32 with 3 colors\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dropout_rate)) # Dropout Rate = 0.5\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRVccALge4p"
      },
      "source": [
        "**Étape 3** : Définir la grille d'hyperparamètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmcT8y2RgvIO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Créer le wrapper KerasClassifier\n",
        "model = KerasClassifier(model=create_model, epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "# Définir la grille d'hyperparamètres\n",
        "param_grid = {\n",
        "    'model__learning_rate': [0.001, 0.01, 0.1],\n",
        "    'model__dropout_rate': [0.3, 0.5, 0.7],\n",
        "    'model__num_filters': [32, 64, 128],\n",
        "    'model__kernel_size': [3, 5]\n",
        "}\n",
        "\n",
        "# Définir GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzesoRHAwHFm"
      },
      "source": [
        "**Étape 4** : Effectuer la recherche de grille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnsmDPLGwWJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a672a2f4-9943-4ad9-c501-8e4d2b538676"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
          ]
        }
      ],
      "source": [
        "# Effectuer la recherche\n",
        "grid_search_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Afficher les meilleurs hyperparamètres trouvés\n",
        "print(\"Meilleurs hyperparamètres : \", grid_search_result.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnHGs7w7wf33"
      },
      "source": [
        "**Étape 5** : Évaluer le modèle optimisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQQT4QwkwoV7"
      },
      "outputs": [],
      "source": [
        "# Extraire les meilleurs hyperparamètres\n",
        "best_params = grid_search_result.best_params_\n",
        "\n",
        "# Créer et entraîner le modèle avec les meilleurs hyperparamètres\n",
        "best_model = create_model(learning_rate=best_params['learning_rate'],\n",
        "                          dropout_rate=best_params['dropout_rate'],\n",
        "                          num_filters=best_params['num_filters'],\n",
        "                          kernel_size=best_params['kernel_size'])\n",
        "\n",
        "history = best_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "# Évaluer le modèle sur le jeu de test\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Accuracy sur le jeu de test : {test_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmuIdDxz2a5f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA-tqpnP1Fd-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFeQIOQdw8YD"
      },
      "source": [
        "**Recherche des paramètres par RandomizedSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgxpeisx1J7j"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Créer le wrapper KerasClassifier\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "# Définir la grille d'hyperparamètres\n",
        "param_dist = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'dropout_rate': [0.3, 0.5, 0.7],\n",
        "    'num_filters': [32, 64, 128],\n",
        "    'kernel_size': [3, 5]\n",
        "}\n",
        "\n",
        "# Définir RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, n_jobs=-1)\n",
        "# Effectuer la recherche\n",
        "random_search_result = random_search.fit(X_train, y_train)\n",
        "\n",
        "# Afficher les meilleurs hyperparamètres trouvés\n",
        "print(\"Meilleurs hyperparamètres : \", random_search_result.best_params_)\n",
        "# Extraire les meilleurs hyperparamètres\n",
        "best_params = random_search_result.best_params_\n",
        "\n",
        "# Créer et entraîner le modèle avec les meilleurs hyperparamètres\n",
        "best_model = create_model(learning_rate=best_params['learning_rate'],\n",
        "                          dropout_rate=best_params['dropout_rate'],\n",
        "                          num_filters=best_params['num_filters'],\n",
        "                          kernel_size=best_params['kernel_size'])\n",
        "\n",
        "history = best_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "# Évaluer le modèle sur le jeu de test\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Accuracy sur le jeu de test : {test_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjp9_JQ3BBSA"
      },
      "source": [
        "**Recherche des paramètres par optimisation bayésienne**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEneNypfBdXm"
      },
      "outputs": [],
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin\n",
        "\n",
        "# Définir l'espace de recherche\n",
        "space = {\n",
        "    'learning_rate': hp.choice('learning_rate', [0.001, 0.01, 0.1]),\n",
        "    'dropout_rate': hp.choice('dropout_rate', [0.3, 0.5, 0.7]),\n",
        "    'num_filters': hp.choice('num_filters', [32, 64, 128]),\n",
        "    'kernel_size': hp.choice('kernel_size', [3, 5])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JpRigutVvv0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def objective(params):\n",
        "    model = create_model(params)\n",
        "\n",
        "    # Split des données d'entraînement pour la validation\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "    # Entraîner le modèle\n",
        "    history = model.fit(X_train_split, y_train_split, epochs=10, batch_size=64,\n",
        "              validation_data=(X_val_split, y_val_split), verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
        "\n",
        "    # Évaluer le modèle\n",
        "    val_loss, val_accuracy = model.evaluate(X_val_split, y_val_split, verbose=0)\n",
        "    return {'loss': -val_accuracy, 'status': STATUS_OK}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOAeS8gzWeGy"
      },
      "outputs": [],
      "source": [
        "# Définir Trials pour stocker les résultats\n",
        "trials = Trials()\n",
        "\n",
        "# Lancer l'optimisation\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "print(\"Meilleurs hyperparamètres : \", best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pac50_LWXDX_"
      },
      "outputs": [],
      "source": [
        "# Traduire les hyperparamètres choisis par Hyperopt\n",
        "best_params = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1][best['learning_rate']],\n",
        "    'dropout_rate': [0.3, 0.5, 0.7][best['dropout_rate']],\n",
        "    'num_filters': [32, 64, 128][best['num_filters']],\n",
        "    'kernel_size': [3, 5][best['kernel_size']]\n",
        "}\n",
        "\n",
        "# Créer et entraîner le modèle avec les meilleurs hyperparamètres\n",
        "best_model = create_model(best_params)\n",
        "history = best_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "# Évaluer le modèle sur le jeu de test\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Accuracy sur le jeu de test : {test_accuracy * 100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}