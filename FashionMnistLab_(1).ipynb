{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EQ_k_RgjNgB"
      },
      "outputs": [],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Étape 1 : Chargement et prétraitement des données\n",
        "# Le dataset Fashion MNIST contient 70 000 images de vêtements en 10 catégories.\n",
        "\n",
        "# Charger les données Fashion MNIST\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Les images sont en niveaux de gris (28x28). Ajoutons une dimension pour correspondre au format attendu par CNN (28x28x1).\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Normalisation entre 0 et 1\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Encodage des labels en one-hot (exemple : 3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Étape 2 : Définition du modèle CNN\n",
        "# On utilise le modèle séquentiel pour empiler les couches.\n",
        "\n",
        "def create_cnn_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Première couche convolutionnelle\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  # Réduction de la taille de l'image\n",
        "\n",
        "    # Deuxième couche convolutionnelle\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Aplatissement des caractéristiques (passage de 2D à 1D)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Couche dense (fully connected) pour l'apprentissage\n",
        "    model.add(Dense(128, activation='relu'))  # 128 neurones\n",
        "    model.add(Dropout(0.5))  # Régularisation pour éviter l'overfitting\n",
        "\n",
        "    # Couche de sortie avec softmax pour la classification en 10 classes\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compilation du modèle\n",
        "    model.compile(\n",
        "        optimizer='adam',  # Optimiseur Adam\n",
        "        loss='categorical_crossentropy',  # Fonction de perte pour la classification multi-classes\n",
        "        metrics=['accuracy']  # Métrique pour évaluer les performances\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Créer le modèle\n",
        "model = create_cnn_model()\n",
        "\n",
        "# Résumé du modèle\n",
        "model.summary()\n",
        "\n",
        "# Étape 3 : Entraînement du modèle\n",
        "# On entraîne le modèle sur les données d'entraînement et on évalue sur les données de test.\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,  # Données d'entraînement\n",
        "    batch_size=64,  # Nombre d'échantillons par batch\n",
        "    epochs=10,  # Nombre d'époques (itérations complètes sur les données)\n",
        "    validation_split=0.2,  # 20% des données d'entraînement utilisées pour la validation\n",
        "    verbose=1  # Affichage détaillé\n",
        ")\n",
        "\n",
        "# Étape 4 : Évaluation sur les données de test\n",
        "# Cela permet de voir la précision du modèle sur des données jamais vues.\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Précision sur les données de test : {test_accuracy:.2f}\")\n",
        "\n",
        "# Étape 5 : Visualisation des courbes d'entraînement (optionnel)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Courbe de perte\n",
        "plt.plot(history.history['loss'], label='Perte d\\'entraînement')\n",
        "plt.plot(history.history['val_loss'], label='Perte de validation')\n",
        "plt.title('Courbe de Perte')\n",
        "plt.xlabel('Époques')\n",
        "plt.ylabel('Perte')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Courbe de précision\n",
        "plt.plot(history.history['accuracy'], label='Précision d\\'entraînement')\n",
        "plt.plot(history.history['val_accuracy'], label='Précision de validation')\n",
        "plt.title('Courbe de Précision')\n",
        "plt.xlabel('Époques')\n",
        "plt.ylabel('Précision')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c3JkGvbxuOvc"
      },
      "outputs": [],
      "source": [
        "# Install Keras Tuner if not already installed (uncomment if needed)\n",
        "!pip install keras-tuner --quiet\n",
        "\n",
        "# Import Keras Tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Redefine the model function to allow hyperparameter tuning\n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional layer with tunable filters\n",
        "    model.add(Conv2D(\n",
        "        filters=hp.Int('conv_1_filters', min_value=16, max_value=64, step=16),  # Tune filters\n",
        "        kernel_size=(3, 3),\n",
        "        activation='relu',\n",
        "        input_shape=(28, 28, 1)\n",
        "    ))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional layer with tunable filters\n",
        "    model.add(Conv2D(\n",
        "        filters=hp.Int('conv_2_filters', min_value=32, max_value=128, step=32),  # Tune filters\n",
        "        kernel_size=(3, 3),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer with tunable units\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),  # Tune dense units\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))  # Tune dropout rate\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model with a tunable optimizer\n",
        "    model.compile(\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd']),  # Tune optimizer\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = kt.Hyperband(\n",
        "    build_cnn_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='tuner_results',\n",
        "    project_name='fashion_mnist_cnn'\n",
        ")\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\"\"\n",
        "Optimal number of filters for the first convolutional layer: {best_hps.get('conv_1_filters')}\n",
        "Optimal number of filters for the second convolutional layer: {best_hps.get('conv_2_filters')}\n",
        "Optimal number of dense units: {best_hps.get('dense_units')}\n",
        "Optimal dropout rate: {best_hps.get('dropout_rate')}\n",
        "Optimal optimizer: {best_hps.get('optimizer')}\n",
        "\"\"\")\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Précision sur les données de test avec le modèle optimisé : {test_accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwpRmwoAmgHs"
      },
      "source": [
        "**Travail à faire:** Optimiser les hyperparamètres (taux d’apprentissage, nombre de filtres, taille des noyaux, etc.) à l’aide de Grid Search, Random Search ou Keras Tuner.\n",
        "Comparer les performances avec des modèles optimisés."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}